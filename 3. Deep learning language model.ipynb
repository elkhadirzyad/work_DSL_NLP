{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Q3VlIMy0Cqg9"
   },
   "source": [
    "# Task 3: build a neural network language model\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "TDX9RGWJCxnv"
   },
   "source": [
    "Let's start by importing the necessary libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ovmSjkTaFKSe"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import io\n",
    "import random\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Note that we will not use keras tokenizer but keep using the same NLTK tokenizer from task 1\n",
    "from nltk.tokenize import WordPunctTokenizer\n",
    "\n",
    "# We use Keras here for simplicity. Replace with your neural network of choice.\n",
    "\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.layers import Embedding, LSTM, Dense, Dropout\n",
    "from keras.optimizers import RMSprop\n",
    "from keras.callbacks import EarlyStopping\n",
    "from keras.models import Sequential\n",
    "from keras.utils import to_categorical\n",
    "\n",
    "# dataframe display option\n",
    "pd.options.display.max_columns = None\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "9T2bA3j3DZdd"
   },
   "source": [
    "# Global parameters\n",
    "\n",
    "Next we setup the following global parameters:\n",
    "\n",
    "Set to train on posts, comment or title, three type of posts in the original corpus\n",
    "* POSTS_TYPE\n",
    "\n",
    "Reduce the volume of posts by filtering on number of tokens in the text and sub sampling \n",
    "* MIN_TOKENS_LEN\n",
    "* MAX_TOKENS_LEN\n",
    "* DF_SAMPLE_COUNT\n",
    "\n",
    "Lower tokens occurence limit \n",
    "* TOKENS_MIN_COUNT\n",
    "\n",
    "Finally, define the length of sequences that will act as input and the sliding window to create the sequences from the list of tokens. \n",
    "\n",
    "* SEQUENCE_WINDOW: smaller window generate a large number of sequences\n",
    "* SEQUENCE_LEN: longer sequences makes it harder to train the neural network\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "VkZe3iYlAVZX"
   },
   "outputs": [],
   "source": [
    "# setup variables\n",
    "\n",
    "POSTS_TYPE = 'post'\n",
    "MIN_TOKENS_LEN = 100\n",
    "MAX_TOKENS_LEN = 200\n",
    "DF_SAMPLE_COUNT = 20000\n",
    "\n",
    "TOKENS_MIN_COUNT = 10\n",
    "\n",
    "SEQUENCE_WINDOW = 4\n",
    "SEQUENCE_LEN = 13"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "NeaJsvhlEqUN"
   },
   "source": [
    "### Load the data\n",
    "We use the same dataset we created in task 1 and used to train n-grams models in task 2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "robj2F7sH2vv"
   },
   "outputs": [],
   "source": [
    "# Load data\n",
    "df_raw = pd.read_csv('../Dataset/stackexchange_812k1.tokenized.csv').sample(frac = 1, random_state = 8).reset_index(drop = True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "1hVX2jF8E5iN"
   },
   "source": [
    "### Reduce the dataset\n",
    "\n",
    "In an ideal world where all the RAM and CPU is available free of charge we would not have to reduce the size of our dataset.\n",
    "\n",
    "If we are using Google Colab, the RAM is limited to 25GB. Reducing the size of the dataset is needed to prevent the notebook from crashing. It laso helps to speed up the succesive runs and experiments. \n",
    "\n",
    "We want to limit the number of items and reduce the overall number of unique tokens.\n",
    "\n",
    "Note: if your code crashes for lack of memory, Colab shows you a link to increase the RAM to 25Gb. But then you have to rerun the notebook from the start.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 89
    },
    "colab_type": "code",
    "id": "P1YpwX2HqPqq",
    "outputId": "c05d7a9e-682e-4ca3-ad51-a790bbe81c96"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "df.shape:  (20000, 7)\n",
      "['I am attempting to estimate . I have expressions for both the conditional expectation and the probability of a, where the probability is Poisson distributed and the conditional expectation is calculated with a relatively expensive recursive formula. As the Poisson distribution has countably infinite support all non-negative integers I cannot calculate all elements of the sum. As the conditional expectation has only a recursive formula no closed form that I know I cannot calculate an integral. The conditional expectation is also increasing in a. My thought would be to truncate, as the probabilities will be single peaked and decreasing quite quickly, but I was unsure as to if there is a guideline on where to truncate or if there is a better solution. Thank you.'\n",
      " \"The metric you describe is in fact very common It's mean absolute error, or MAE. In scikit learn you can find it in the metrics submodule . Usually it's used for regression tasks, not for classification, thus you might not have encountered it. Still, when it does get used to compare classification algorithms there are certain caveats, for example For example, it has similar problems like accuracy when used with unbalanced datasets in that it will produce high scores for algorithms that just predict the majority class and thus are not useful . The MAE doesn't tell you if your classifier better at predicting positives or negatives. So to answer your question It is a common, useful scoring metric, but less often used for classifiers more common for regressors .\"]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "df = df_raw[\n",
    "            (df_raw.category == POSTS_TYPE) & \n",
    "            (df_raw.n_tokens > MIN_TOKENS_LEN)  & \n",
    "            (df_raw.n_tokens < MAX_TOKENS_LEN)\n",
    "        ].sample(DF_SAMPLE_COUNT).reset_index(drop = True)\n",
    "\n",
    "print(\"df.shape: \", df.shape)\n",
    "\n",
    "print(df.text.sample(2).values)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "fOdniCO_IYVP"
   },
   "source": [
    "If you recall, the tokens are stored in the dataframe as strings and separated by whitespaces. Let's transform back the tokens column into arrays of tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 451
    },
    "colab_type": "code",
    "id": "zEL8R16zqxHi",
    "outputId": "7c567443-157d-4791-dc49-e56d6d83f4b5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[array(['is', 'this', 'much', 'different', 'from', 'doing', 'two', 'local',\n",
      "       'polynomials', 'of', 'degree', ',', 'one', 'for', 'below', 'the',\n",
      "       'threshold', 'and', 'one', 'for', 'above', 'with', 'smooth', 'at',\n",
      "       'are', 'the', 'upper', 'and', 'lower', 'limits', 'of', 'the',\n",
      "       'confidence', 'interval', 'for', 'the', 'smoothed', 'outcome', '.',\n",
      "       'lpoly', 'lne', 'd', 'if', 'd', 'lt', ',', 'bw', '.', 'deg', 'n',\n",
      "       'gen', 'x', 's', 'ci', 'se', 'se', 'lpoly', 'lne', 'd', 'if', 'd',\n",
      "       'gt', ',', 'bw', '.', 'deg', 'n', 'gen', 'x', 's', 'ci', 'se',\n",
      "       'se', 'get', 'the', 'cis', 'forvalues', 'v', 'gen', 'ul', 'v', \"'\",\n",
      "       's', 'v', \"'\", '.', 'se', 'v', \"'\", 'gen', 'll', 'v', \"'\", 's',\n",
      "       'v', \"'\", '-', '.', 'se', 'v', \"'\", 'tw', 'line', 'ul', 'll', 's',\n",
      "       'x', ',', 'lcolor', 'blue', 'blue', 'blue', 'lpattern', 'dash',\n",
      "       'dash', 'solid', 'line', 'ul', 'll', 's', 'x', ',', 'lcolor',\n",
      "       'red', 'red', 'red', 'lpattern', 'dash', 'dash', 'solid', ',',\n",
      "       'legend', 'off', 'as', 'you', 'can', 'see', ',', 'the', 'lines',\n",
      "       'in', 'the', 'first', 'plot', 'are', 'the', 'same', 'as', 'in',\n",
      "       'the', 'second', '.'], dtype='<U11')]\n"
     ]
    }
   ],
   "source": [
    "# transform the tokens field from white space separated strings into list of tokens\n",
    "df['tokens'] = df.tokens.apply(lambda t : np.array(t.split()))\n",
    "print(df.tokens.sample().values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ZMErst9kGLVx"
   },
   "source": [
    "## Vocabulary\n",
    "\n",
    "Let's new reduce the overall vocabulary size by excluding tokens that appear less than TOKENS_MIN_COUNT overall.\n",
    "\n",
    "The goal is to reduce the number of unique tokens with a minimum of impact on the original list of tokens.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 86
    },
    "colab_type": "code",
    "id": "Sic9Em6RS9YR",
    "outputId": "2dd1918c-4a76-40e7-d578-69a95f211953"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "original number of tokens 2859381\n",
      "original vocab_size 32864\n",
      "nomber of tokens 2796260\n",
      "vocab_size 7226\n"
     ]
    }
   ],
   "source": [
    "# generate vocabulary\n",
    "# filter out words that are too scarce\n",
    "import itertools\n",
    "all_tokens = list(itertools.chain.from_iterable(df.tokens))\n",
    "\n",
    "# filter out least common tokens\n",
    "from collections import Counter\n",
    "counter_tokens = Counter(all_tokens)\n",
    "\n",
    "\n",
    "vocab_size  = len(set(all_tokens))\n",
    "vocab       = list(set(all_tokens))\n",
    "print(\"original number of tokens\", len(all_tokens))\n",
    "print(\"original vocab_size\", vocab_size)\n",
    "\n",
    "\n",
    "# remove all tokens that appear in less than TOKENS_MIN_COUNT times\n",
    "fltrd_tokens = [ token for token in all_tokens if counter_tokens[token] > TOKENS_MIN_COUNT ]\n",
    "\n",
    "print(\"nomber of tokens\", len(fltrd_tokens))\n",
    "print(\"vocab_size\", len(set(fltrd_tokens)))\n",
    "\n",
    "vocab_size  = len(set(fltrd_tokens))\n",
    "vocab       = list(set(fltrd_tokens))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "IuNDtkRuHGtJ"
   },
   "source": [
    "Just to be sure let's inspect the tokens that were rejected"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "JoSmBqj6HG5U"
   },
   "outputs": [],
   "source": [
    "# rejected tokens\n",
    "\n",
    "rejected_tokens = np.unique([ token for token in all_tokens if counter_tokens[token] <= TOKENS_MIN_COUNT ])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 312
    },
    "colab_type": "code",
    "id": "oTQWUUI0JTf3",
    "outputId": "d9bdb937-5c79-4267-ba96-839c0d479c77"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "len(rejected_tokens):  25638\n",
      "['warping' 'ailment' 'foldcv' 'ig' 'faculty' 'galaxies' 'pycm' 'workbook'\n",
      " 'mushroom' 'propper' 'procedur' 'keys' 'voxel' 'surveyna' 'pointest'\n",
      " 'remarks' 'glivenko' 'lohr' 'coxtest' 'aces' 'hθ' 'theonull'\n",
      " 'binarization' 'sharks' 'memspace' 'erratically' 'sizing' 'fevers' 'loaf'\n",
      " 'chnaged' 'yoy' 'arguable' 'newmod' 'niceness' 'prp' 'eclipse'\n",
      " 'arrangement' 'ive' 'werksituatie' 'eigenfaces' 'spring' 'dimitriy'\n",
      " 'vglm' 'hodg' 'emailing' 'discounted' 'islucky' 'preferrably'\n",
      " 'indifferent' 'minimally' 'guidiance' 'hyvarinen' 'depths' 'fitdiscr'\n",
      " 'sidebar' 'expectantly' 'bowl' 'mobile' 'cplot' 'pior' 'handicap'\n",
      " 'erlang' 'oristano' 'constuct' 'balloon' 'tunçel' 'assisting'\n",
      " 'undersample' 'inlier' 'indefinite' 'bowling' 'vicious' 'shits' 'dietz'\n",
      " 'flowchart' 'reinforce' 'expicitly' 'wps' 'julie' 'stazionary'\n",
      " 'overfiting' 'clc' 'carcinus' 'hakan' 'biggiest' 'nonsignificant'\n",
      " 'killmann' 'bolded' 'binomila' 'determinable' 'accuray' 'vsports'\n",
      " 'fulfil' 'clerks' 'disagreed' 'dietterich' 'transliterated' 'nicolas'\n",
      " 'conversations' 'runtimes']\n"
     ]
    }
   ],
   "source": [
    "print(\"len(rejected_tokens): \", len(rejected_tokens))\n",
    "print(np.random.choice(rejected_tokens, 100, replace = False))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "buizLAhOKGTH"
   },
   "source": [
    "# OOV\n",
    "Next we need to replace the missing tokens by a specific token so that our model knows how to handle OOV. \n",
    "\n",
    "Let's add the token \"UNK\" to the vocabulary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "haR6oBJldOSm"
   },
   "outputs": [],
   "source": [
    "vocab.append('UNK')\n",
    "vocab_size +=1 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "wGascZIGKXH-"
   },
   "source": [
    "## tokens as vocabulary indexes\n",
    "\n",
    "At this point, we have sequences of tokens and we need sequences of numbers. \n",
    "We replace each token by its index in the vocabulary making sure that unknown tokens are replaced by the index of the \"UNK\" word.\n",
    "\n",
    "This step can take quite a while to run when the size of the dataframe is too large.\n",
    "\n",
    "To speed up this step and still handle OOV tokens it's best to avoid conditions in the list expression and use a try / except pattern in a function.\n",
    "[link text](https://)    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "gL2HpO25NFqZ"
   },
   "outputs": [],
   "source": [
    "mapping = { w : i for i, w in enumerate(vocab) }\n",
    "\n",
    "def getidx(token):\n",
    "    try:\n",
    "        return mapping[token]\n",
    "    except:\n",
    "        return mapping['UNK']\n",
    "\n",
    "\n",
    "df['tokens_idx'] = df.tokens.apply(lambda tokens : np.array([ getidx(token) for token in tokens]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 521
    },
    "colab_type": "code",
    "id": "Wfd9IlO5K8Eo",
    "outputId": "cc1f6d1f-7515-4872-901c-ba39f3d05969"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[array([5356, 3153,  635, 1734, 6917, 7125, 6174, 2198, 1561, 3534, 4479,\n",
      "       3101, 4085, 4642, 1807, 4642, 3310, 3271, 2881, 4599, 3415, 3101,\n",
      "       5880, 1807, 3009, 5369, 2900, 6078, 5282, 5708, 5967, 2810, 1962,\n",
      "       3495, 2623, 3153, 1513, 5349,  788, 1081, 1867, 1203, 3153, 2140,\n",
      "       5159, 5042, 3153, 2140, 1858, 5967, 5904, 2221, 3747, 3153, 5297,\n",
      "       2881, 3521, 3239, 3101, 1787, 1962, 6203, 5955, 2521, 1685, 1513,\n",
      "       5349,  788, 2948, 2175, 6114, 3101, 1136, 1962, 2138, 5369,  839,\n",
      "       5904, 1081, 3436, 5967, 6885, 3239, 3432, 4640, 5972, 2221, 3747,\n",
      "       2948, 1081, 3153, 5822, 3239, 5955, 5752, 1391, 3748, 2322, 3101,\n",
      "       5644, 1174, 2221, 1792, 5469, 2652, 2342, 2948, 5822, 4877, 1787,\n",
      "       5369, 6792, 5449, 3101, 2175, 1081, 4624, 3473, 2057, 5459, 5297,\n",
      "       2881, 5822, 4877, 5967, 2623, 3473, 5459, 4801, 4433,  981, 3748,\n",
      "       3635, 5955, 3741, 1685, 3799, 3239, 2707, 3101, 6917, 3261, 5955,\n",
      "       3070, 3368, 3101, 2175, 1081, 1014, 7226, 3748, 3432, 3153, 5297,\n",
      "       2881, 3521, 3239, 3101, 5850, 5873, 2698, 5473,  391, 5967, 4526,\n",
      "       3101, 3799, 4877, 1685, 5955, 5953, 2881, 6174, 3239, 2652, 1081,\n",
      "       1632, 3748, 5494, 5904,  569, 1081, 3153, 1318, 2261, 5955, 1298,\n",
      "       5994,   79, 5369, 6353, 5955, 6174, 3101, 6917, 5955, 5994,   79,\n",
      "       5967])\n",
      " array([4642, 5298, 3412, 2942, 2948, 5904, 5880, 3101, 4640,  697, 4549,\n",
      "       2208, 3092, 2900, 3152, 3153, 4082, 3521, 3239, 3101, 6917, 5091,\n",
      "       4640, 1907, 2322, 3514, 3261, 3153, 5297, 2881, 3521, 3965, 1987,\n",
      "       3153, 6547, 3239, 5693, 5013, 1685, 5955, 5301, 3101, 2221, 2208,\n",
      "       1849, 1651, 1962, 4640, 2208, 3951,  239, 3473, 2298,  981,  256,\n",
      "       4587, 3153, 3583, 5297, 2881, 4082, 3521, 3965, 5967, 4192, 3101,\n",
      "       1319, 4114, 4640,  254, 6638, 3153, 5839, 2007, 7020, 3153,  132,\n",
      "       1685, 3195, 6864, 5967, 3136, 4001, 2592, 5563, 3101, 4640,  254,\n",
      "       6917, 1014, 3009, 3747, 3153, 5862, 6981, 3879, 1962, 2492, 4372,\n",
      "       3261, 7226, 5195,  788, 5967, 5494, 7226, 5967])]\n"
     ]
    }
   ],
   "source": [
    "print(df.tokens_idx.head(2).values)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "nSOucX9ZLBow"
   },
   "source": [
    "## Sequence generation\n",
    "\n",
    "This is the final step in preparing the corpus as input to the neural network.\n",
    "\n",
    "We want to train the neural net on a classification task where the input is a sequence of words (as token indexes) and the output the following word.\n",
    "\n",
    "From each list of tokens indexes, we generate K sequences of length N by taking a subset of length SEQUENCE_LENGTH and repeatedly sliding the window by SEQUENCE_WINDOW.\n",
    "\n",
    "For instance, in a sentence of 15 tokens, if we set SEQUENCE_LENGTH = 6 and SEQUENCE_WINDOW = 3, we generate 5 sequences of length 6. Sequences that are shorter than SEQUENCE_LENGTH are left-padded with zeros.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "DJE2AMR7PKCV"
   },
   "outputs": [],
   "source": [
    "\n",
    "def generate_sequences(sentence):\n",
    "    sequences = []\n",
    "    _end = SEQUENCE_WINDOW\n",
    "    while _end < len(sentence) + SEQUENCE_WINDOW:\n",
    "        sequences.append(sentence[:_end])\n",
    "        _end += SEQUENCE_WINDOW\n",
    "    padded_seqs = pad_sequences(sequences, maxlen=SEQUENCE_LEN, padding='pre')\n",
    "    return padded_seqs\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "VJhU4VWa0g3c"
   },
   "outputs": [],
   "source": [
    "# Apply the sequence generation \n",
    "multi_sequences = df.tokens_idx.apply(generate_sequences)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "hj9UXQDC5Zak"
   },
   "source": [
    "The code below can be optimized to avoid reallocating memory as the main array is expanded with each new array of sequences. But for now it will suffice."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 69
    },
    "colab_type": "code",
    "id": "cLMn2QDW1i18",
    "outputId": "4086db40-4b71-43d4-8d18-f0bd265a4601"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████| 20000/20000 [03:49<00:00, 87.29it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "sequences.shape:  (722440, 13)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "i = 0\n",
    "for d in tqdm(multi_sequences.values):\n",
    "    if i == 0:\n",
    "        all_sequences = d\n",
    "    else:\n",
    "        all_sequences = np.concatenate( ( all_sequences, d )  )\n",
    "    i +=1\n",
    "print(\"\\nsequences.shape: \",all_sequences.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "nHoTt-IeOd7u"
   },
   "source": [
    "Depending on your parameters, this may result in a massive amount of sequences. Although more data is always best, having too many sequences will make it hard and time consuming to train the network. \n",
    "\n",
    "The next step is optional but shows how to sample N% of the sequences to reduce the input dataset. \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "OypLEP1-SGqO"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "sequences.shape:  (361231, 13)\n"
     ]
    }
   ],
   "source": [
    "if True:\n",
    "    \n",
    "    mask = np.random.choice([False, True], len(all_sequences), p=[0.50, 0.50])\n",
    "\n",
    "    sequences = all_sequences[mask].copy()\n",
    "else:\n",
    "    sequences = all_sequences.copy()\n",
    "    print(\"\\nsequences.shape: \",sequences.shape)\n",
    "print(\"\\nsequences.shape: \",sequences.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "jJum_ZPOo-Tx"
   },
   "source": [
    "Then we create the predictors and labels for the classificaton task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 69
    },
    "colab_type": "code",
    "id": "KnqSNmU80VKX",
    "outputId": "dad060d7-7014-4a88-82b1-e4df95d02825"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "predictors.shape (361231, 12)\n",
      "label.shape (361231,)\n",
      "label_cat.shape (361231, 7227)\n"
     ]
    }
   ],
   "source": [
    "predictors  = sequences[:,:-1]\n",
    "label       = sequences[:,-1]\n",
    "\n",
    "print(\"predictors.shape\", predictors.shape)\n",
    "print(\"label.shape\", label.shape)\n",
    "\n",
    "# The to_categorical Keras function transforms the vocab_size vector of labels into a one hot encoded matrix of dimension (n, vocab_size)\n",
    "label_cat       = to_categorical(label, num_classes=vocab_size)\n",
    "\n",
    "print(\"label_cat.shape\", label_cat.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "hmyKzT87pbSG"
   },
   "source": [
    "# Model\n",
    "\n",
    "We are now ready to define and train the neural network.\n",
    "\n",
    "We choose\n",
    "\n",
    "* an embedding dimension (32, 64, ...), \n",
    "* 2 LSTM layers \n",
    "* followed by a dense layer with softmax activation\n",
    "* the optimizer is RMSprop with a learning rate of 0.01\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 312
    },
    "colab_type": "code",
    "id": "ndIsWXx4FelC",
    "outputId": "0d0cc35b-a538-4492-ec4d-dc9944bf21ab"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_1 (Embedding)      (None, 12, 64)            462528    \n",
      "_________________________________________________________________\n",
      "lstm_1 (LSTM)                (None, 12, 128)           98816     \n",
      "_________________________________________________________________\n",
      "lstm_2 (LSTM)                (None, 64)                49408     \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 7227)              469755    \n",
      "=================================================================\n",
      "Total params: 1,080,507\n",
      "Trainable params: 1,080,507\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "Define model\n",
    "'''\n",
    "embedding_dimension = 64\n",
    "model = Sequential()\n",
    "model.add(\n",
    "    Embedding(vocab_size,\n",
    "        embedding_dimension,\n",
    "        input_length=SEQUENCE_LEN -1)\n",
    "    )\n",
    "model.add(LSTM(128, return_sequences = True))\n",
    "model.add(LSTM(64))\n",
    "model.add(Dense(vocab_size, activation='softmax'))\n",
    "optimizer = RMSprop(lr=0.01)\n",
    "\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "    optimizer=optimizer,\n",
    "    metrics=['accuracy'])\n",
    "\n",
    "print(model.summary())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "dNYH5r1pp9dB"
   },
   "source": [
    "Let's fit the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 173
    },
    "colab_type": "code",
    "id": "ea8KyX_-HhjZ",
    "outputId": "4c41ee08-724c-4c96-f497-c225c6512797"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\zyad\\Anaconda3\\lib\\site-packages\\tensorflow_core\\python\\framework\\indexed_slices.py:433: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/4\n",
      "361231/361231 [==============================] - 237s 655us/step - loss: 5.6056 - accuracy: 0.1335\n",
      "Epoch 2/4\n",
      "361231/361231 [==============================] - 225s 622us/step - loss: 5.3661 - accuracy: 0.1828\n",
      "Epoch 3/4\n",
      "361231/361231 [==============================] - 221s 612us/step - loss: 5.3116 - accuracy: 0.1978\n",
      "Epoch 4/4\n",
      "   512/361231 [..............................] - ETA: 2:15:19 - loss: 4.8694 - accuracy: 0.1816"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\zyad\\Anaconda3\\lib\\site-packages\\keras\\callbacks\\callbacks.py:95: RuntimeWarning: Method (on_train_batch_end) is slow compared to the batch update (0.371191). Check your callbacks.\n",
      "  % (hook_name, delta_t_median), RuntimeWarning)\n",
      "C:\\Users\\zyad\\Anaconda3\\lib\\site-packages\\keras\\callbacks\\callbacks.py:95: RuntimeWarning: Method (on_train_batch_end) is slow compared to the batch update (0.203831). Check your callbacks.\n",
      "  % (hook_name, delta_t_median), RuntimeWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "361231/361231 [==============================] - 235s 651us/step - loss: 5.2655 - accuracy: 0.2021\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.callbacks.History at 0x1d3459cff60>"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "Model Fitting!\n",
    "'''\n",
    "model.fit(predictors, label_cat, batch_size = 256, epochs=4, verbose=1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ZFJwL0MrRUNi"
   },
   "outputs": [],
   "source": [
    "def sample(preds, temperature=1.0):\n",
    "    # helper function to sample an index from a probability array\n",
    "    preds = np.asarray(preds).astype('float64')\n",
    "    preds = np.log(preds) / temperature\n",
    "    exp_preds = np.exp(preds)\n",
    "    preds = exp_preds / np.sum(exp_preds)\n",
    "    probas = np.random.multinomial(1, preds, 1)\n",
    "    return np.argmax(probas)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "AY-_6qDhGMRz"
   },
   "outputs": [],
   "source": [
    "\n",
    "tokenizer = WordPunctTokenizer()\n",
    "\n",
    "def generate_text(nmax, text, temperature):\n",
    "    n = 0\n",
    "    tokens = tokenizer.tokenize(text)\n",
    "    while (len(tokens) < nmax) :\n",
    "        n +=1\n",
    "        \n",
    "        # only takes known words into account\n",
    "        tokens_idx = [ vocab.index(word) if word in vocab else vocab.index('UNK') for word in tokens  ]\n",
    "        # print(tokens_idx)\n",
    "        tokens_list = pad_sequences([tokens_idx], maxlen=SEQUENCE_LEN-1, padding='pre')\n",
    "        probas = model.predict_proba(tokens_list, verbose=0)[0]\n",
    "\n",
    "        next_word_idx = sample(probas, temperature = temperature)\n",
    "        next_word = vocab[next_word_idx]\n",
    "        # print(next_word_idx, next_word)\n",
    "\n",
    "        # next_word = np.random.choice(vocab, p = probas)\n",
    "        if next_word != '?':\n",
    "            print(next_word, probas[vocab.index(next_word)]  )\n",
    "            text += ' ' + next_word\n",
    "        # print(text)\n",
    "        tokens = tokenizer.tokenize(text)\n",
    "        if n> 200:\n",
    "            break;\n",
    "    return text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "P8iUDO4H0Lc5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "kpss 1.9849795e-06\n",
      "penalized 1.5628754e-06\n",
      "ssa 2.096636e-09\n",
      "given 0.0020274164\n",
      "aicc 1.5821489e-06\n",
      "student 0.00035136475\n",
      "color 7.966885e-06\n",
      "faces 3.1813456e-06\n",
      "documents 0.00021886693\n",
      "e 0.0030902135\n",
      "high 8.537658e-06\n",
      "cronbach 9.656214e-08\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'a random variable kpss penalized ssa given aicc student color faces documents e high cronbach'"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generate_text(15, 'a random variable', 3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "fnSYB2uW6dex"
   },
   "source": [
    "# Perplexity\n",
    "\n",
    "In the case of the n-gram language model, we used the propability of each n-gram in the input sentence to calculate the perplexity. \n",
    "\n",
    "Our current model does not rely on n-grams, but on probabilities of sequences of tokens to be followed by a subsequent token.\n",
    "\n",
    "We can adapt the perplexity formula for n-grams language models to sequence based language models as such\n",
    "\n",
    "if we consider the sentence of N tokens: \n",
    "\n",
    "$$w_{1},\\cdots, w_N$$\n",
    "\n",
    "Then we can calculate the probability of that sentence as the product of probabilities of all the padded subsequences. Let's take an example of a 3 tokens sentence.\n",
    "\n",
    "$$\n",
    "P(w_{1},w_2, w_3) =  P(w_{3} | w_1, w_2) \\times p(w_2 | w_1 ,0)  \\times p(w_1 | 0 ,0)\n",
    "$$\n",
    "\n",
    "In general, for a sentence of N tokens and a sequence length of length S \n",
    "\n",
    "$$\n",
    "P(w_{1},\\cdots, w_N) = \\prod_{k = 1}^{ \\max{(N,S)}} P(w_{k} | \\text{padded}_S(w_{1}, \\cdots, w_{k-1})    ) \n",
    "$$\n",
    "\n",
    "where \n",
    "$$P(w_{k} | \\text{padded}_S(w_{1}, \\cdots, w_{k-1})$$ \n",
    "\n",
    "is precisely the probability given by the classification model.\n",
    "\n",
    "We can compute the perplexity of a sentence of length N with \n",
    "$$PP(w_{1},\\cdots, w_N) = \\exp [ - \\frac{1}{N} {\\sum_{i = 1}^{ \\max{(N,S)} } \\log { P(w_{k} | \\text{padded}_S(w_{1}, \\cdots, w_{k-1}) } } ) ]$$\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "jfjMIreA3Req"
   },
   "outputs": [],
   "source": [
    "# set the sequence window to 1 to generate all the sub sequences from the original sentence.\n",
    "\n",
    "SEQUENCE_WINDOW = 1\n",
    "\n",
    "# and define the perplexity for a sentence\n",
    "\n",
    "def perplexity(sentence):\n",
    "    # tokenize\n",
    "    tokens = tokenizer.tokenize(sentence.lower())\n",
    "    N = len(tokens)\n",
    "    # find the indexes of the tokens from the vocabulary\n",
    "    tokens_idx = [ vocab.index(word) if word in vocab else vocab.index('UNK') for word in tokens  ]\n",
    "    # generate a N x SEQUENCE_LEN array of padded sequences \n",
    "    sequences = generate_sequences(tokens_idx)\n",
    "    predictors  = sequences[:,:-1]\n",
    "    label       = sequences[:,-1]\n",
    "    # the probabilities of all the words in the vocab given each padded sequence\n",
    "    probas = model.predict_proba(predictors, verbose=0)\n",
    "    # add the log of the probability of the label given the padded sequence\n",
    "    logprob = 0\n",
    "    for k in range(N):\n",
    "        p = probas[k,label[k]]\n",
    "        logprob += np.log( p  )    \n",
    "    return np.exp(- logprob / N), logprob\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ngyLoOSXAjo6"
   },
   "source": [
    "Now compare three sentences, the first one directly extracted from the corpus, the second from an old song and the third gammaticaly invalid.\n",
    "\n",
    "Although this is not direct proof of the quality of the model, we see that the 1st sentence has the lowest perplexity, while the 3rd sentence the highest. And the second sentence, which is grammaticaly correct but obviously not about data or algorithm scores in between.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 69
    },
    "colab_type": "code",
    "id": "a8NjxKK258jF",
    "outputId": "1bad57bb-f04c-4c53-e969-5ba5075b0741"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "In a fixed-effects model only time-varying variables can be used. (243.19416503047952, -82.40790235996246)\n",
      "I know a pretty little place in Southern California, down San Diego way. (374.04122297881037, -88.86549019813538)\n",
      "This that is noon but yes apple whatever did regression variable (3454.791916884255, -89.62269258499146)\n"
     ]
    }
   ],
   "source": [
    "sentence = \"In a fixed-effects model only time-varying variables can be used.\"\n",
    "print(sentence, perplexity(sentence))\n",
    "\n",
    "sentence = \"I know a pretty little place in Southern California, down San Diego way.\"\n",
    "print(sentence, perplexity(sentence))\n",
    "\n",
    "sentence = \"This that is noon but yes apple whatever did regression variable\"\n",
    "print(sentence, perplexity(sentence))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "OCSz9AM8EghI"
   },
   "source": [
    "## Perplexity on corpus\n",
    "\n",
    "Finally let's calculate the perplexity on a validation set.\n",
    "\n",
    "We define the validation set as N random items from the original corpus. Here we will choose 100 titles with between 10 and 100 tokens.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 225
    },
    "colab_type": "code",
    "id": "rMpjSmqhE5GD",
    "outputId": "32edd699-8bbb-4841-b376-c33f540be5f2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    post_id  parent_id  comment_id  \\\n",
      "21   154700        NaN         NaN   \n",
      "24   160640        NaN         NaN   \n",
      "\n",
      "                                                 text category  \\\n",
      "21  Are aov with Error same as lmer of lme package...    title   \n",
      "24  How to compare contingency tables for a specif...    title   \n",
      "\n",
      "                                               tokens  n_tokens  \n",
      "21  are aov with error same as lmer of lme package...        13  \n",
      "24  how to compare contingency tables for a specif...        10  \n"
     ]
    }
   ],
   "source": [
    "df_valid = df_raw[df_raw.category.isin(['title'])].copy()\n",
    "#print(\"df_valid\",df_valid)\n",
    "print(df_valid.head(2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "LnyFe2nO0W1K"
   },
   "outputs": [],
   "source": [
    "def corpus_perplexity(corpus):\n",
    "    # start by calculating the total number of tokens in the corpus\n",
    "    all_sentences = ' '.join(corpus)\n",
    "    all_tokens =  tokenizer.tokenize(all_sentences.lower())\n",
    "    N = len(all_tokens)\n",
    "    logproba = 0\n",
    "    perps = []\n",
    "    for sentence in corpus:\n",
    "        pp, logp = perplexity(sentence)\n",
    "        logproba += logp\n",
    "        perps.append(pp)\n",
    "        #print (\"{:.2f}\\t{:.2f}\\t{:.2f}\\t{:.2f}\\t{:.2f}\\t{}\".format(pp, np.mean(perps), logp, logproba, np.exp( - logproba / (N  )), sentence  ))\n",
    "\n",
    "    return np.exp( - logproba / (N))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "ZXxDMjxn0dmp",
    "outputId": "738dff4b-4da9-47ef-c239-984dd4b18329"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Corpus perplexity: 659.78\n"
     ]
    }
   ],
   "source": [
    "corpus = df_valid.text.values\n",
    "#print(corpus)\n",
    "perplexity_score= corpus_perplexity(corpus)\n",
    "print(\" Corpus perplexity: {:.2f}\".format(perplexity_score ))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "machine_shape": "hm",
   "name": "LM word Keras LSTM V03 - Task 3",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
