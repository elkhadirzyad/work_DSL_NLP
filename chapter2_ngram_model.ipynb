{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "sfXBcMPOpJ02"
   },
   "source": [
    "# 1) Loading the dataset & Split the dataset into a training and a testing subset. Use the category “title” for the testing set and the categories “comment” and “post” for the training set. .\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "H69DABm5qMgt"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import csv\n",
    "from tqdm import tqdm\n",
    "from collections import defaultdict, Counter\n",
    "from nltk.util import ngrams\n",
    "from decimal import Decimal, getcontext\n",
    "getcontext().prec = 320"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "wV1Qzr-TrYas"
   },
   "outputs": [],
   "source": [
    "# Set some global parameters\n",
    "\n",
    "# Displaying all columns when displaying dataframes\n",
    "pd.options.display.max_columns = None\n",
    "\n",
    "# We will work with trigrams \n",
    "ngrams_degree = 3\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "yuV6ny4UvZqf"
   },
   "outputs": [],
   "source": [
    "# if there's a problem with the versions of the librairies, you can . . uncomment this line and install the proper versions\n",
    "\n",
    "# !pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "fHoDlqr_qPub"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "TL9EncUpxTN4"
   },
   "source": [
    "we save the tokens : "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "T3eW9r6erVG1"
   },
   "outputs": [],
   "source": [
    "df['tokens'] = df.tokens.apply(lambda txt : txt.split())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Uqjl-9BxdAWm"
   },
   "source": [
    "We split the dataset into a training and a testing subset. \n",
    "\n",
    "The testing subset contains just titles, the train subset contains posts and comments\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "VYlDBQtadOfU"
   },
   "outputs": [],
   "source": [
    "df_train = df[df.category.isin(['post','comment'])].copy()\n",
    "df_test = df[df.category.isin(['title'])].copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "oegvQAzqyyST"
   },
   "source": [
    "# 2) Build the matrix of prefix—word frequencies.\n",
    "\n",
    "     Use the ngrams function from nltk.utils to generate all n-grams from the corpus\n",
    "         & Set the following left_pad_symbol = <s> and right_pad_symbol = </s>\n",
    "\n",
    "\n",
    "2-1) We use the [ntlk.ngrams](https://www.nltk.org/api/nltk.html#nltk.util.ngrams) function to split the tokens into bigramns\n",
    "\n",
    "\n",
    "    The counts object will have the bigrams as keys and for each key a Counter of all the potential tokens. \n",
    "\n",
    "\n",
    "\n",
    "    Note : We will work on training subset df_train and leave the testing subset aside.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "L8cvgLCysPZR",
    "outputId": "ed36d02c-4bee-41bb-d7da-ac9a01f5aa14"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████| 705964/705964 [02:03<00:00, 5711.27it/s]\n"
     ]
    }
   ],
   "source": [
    "counts = defaultdict(Counter)\n",
    "for tokens in tqdm(df_train.tokens.values):\n",
    "    for ngram in ngrams(\n",
    "          tokens, \n",
    "          n= ngrams_degree,  \n",
    "          pad_right=True, \n",
    "          pad_left=True, \n",
    "          left_pad_symbol=\"<s>\", \n",
    "          right_pad_symbol=\"</s>\"):\n",
    "      \n",
    "        prefix = ngram[:ngrams_degree-1]\n",
    "        token = ngram[ngrams_degree-1]\n",
    "        counts[prefix][token] +=1\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "a717NY_i3cyK"
   },
   "source": [
    "\n",
    "\n",
    "2-2) To obtain token / prefix probabilities we use the Maximum Likelihood Estimator\n",
    "\n",
    "     $$p(token / prefix) = \\frac{count(prefix + token)} {count(prefix)}$$\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Lt1QYcHlssWI"
   },
   "outputs": [],
   "source": [
    "freq = defaultdict(dict)\n",
    "for prefix, tokens in counts.items():\n",
    "    total = sum( counts[prefix].values()  )\n",
    "    for token, c in tokens.items():\n",
    "        freq[prefix][token] = c / total\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "70ZlWvA54_JF"
   },
   "source": [
    "# 3) Write a text generation function\n",
    "\n",
    "    3-1) takes a bigram as input and generates the next token\n",
    "    3-2) iteratively slide the prefix over the generated text so that the new prefix includes the most recent token; \n",
    "    3-3) generates the next token to generate each next token, sample the list of words associated with the prefix using the\n",
    "         probability distribution of the prefix stop the text generation when a certain number of words have been generated or\n",
    "         the latest token is a </s>\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "OGlrLVja4zC9"
   },
   "outputs": [],
   "source": [
    "def generate(text, n_words = 40):\n",
    "    for i in range(n_words):\n",
    "        prefix = tuple(text.split()[-ngrams_degree+1:])\n",
    "        # no available text\n",
    "        if len(freq[prefix]) == 0:\n",
    "            break\n",
    "        candidates  = list(freq[prefix].keys())\n",
    "        probas      = list(freq[prefix].values())\n",
    "        text       += ' ' + np.random.choice(candidates, p = probas)\n",
    "        if text.endswith('</s>'):\n",
    "            break\n",
    "\n",
    "    return text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "the model and the adjusted means because i wasn ' t used hierarchical and not going to get the same . think of using a predictor in a calculation error . </s>\n",
      "to determine if they met the model . i certainly don ' t see it as an attribute that goes to infinity ? are there any way to validate this concept . </s>\n",
      "that distribution . for confidence . </s>\n"
     ]
    }
   ],
   "source": [
    "text      = 'the model'\n",
    "print()\n",
    "print(generate(text))\n",
    "text      = 'to determine'\n",
    "print(generate(text))\n",
    "text      = 'that distribution'\n",
    "print(generate(text))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "jQ4OxyWU_RTG"
   },
   "source": [
    "# 4) Implement the perplexity scoring function for a given sentence and for the        training corpus\n",
    "\n",
    ".\n",
    "  4-1) To avoid the problem of underflow caused by multiplying very small floats, we work in the log space:\n",
    "       So instead of calculating perplexity with (case ngrams_degree = 3):\n",
    " \n",
    "$$PP(w_{1},\\cdots, w_N) = ( \\prod_{i = 3}^{N} \\frac{1}{ p(w_i/ w_{i-2}w_{i-1} )} )^{\\frac{1}{N}}$$\n",
    "\n",
    "We compute\n",
    "\n",
    "$$PP(w_{1},\\cdots, w_N) = \\exp [ - \\frac{1}{N} {\\sum_{i = 3}^{N} \\log {p(w_i/ w_{i-2}w_{i-1}} } ) ]$$\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "sBaXy5fJ-SgY"
   },
   "outputs": [],
   "source": [
    "from nltk.tokenize import WordPunctTokenizer\n",
    "\n",
    "tokenizer = WordPunctTokenizer()\n",
    "\n",
    "\n",
    "def perplexity(sentence):\n",
    "    sentence = tokenizer.tokenize(sentence.lower())\n",
    "    N = len(sentence)\n",
    "    logprob = 0\n",
    "    \n",
    "    for ngram in ngrams(\n",
    "          sentence, \n",
    "          n= ngrams_degree,  \n",
    "          pad_right=True, pad_left=True, \n",
    "          left_pad_symbol=\"<s>\", right_pad_symbol=\"</s>\"):\n",
    "        try:\n",
    "          prefix = ngram[:ngrams_degree-1] \n",
    "          token = ngram[ngrams_degree-1]\n",
    "          logprob += np.log( freq[ prefix ][token]  )\n",
    "        except:\n",
    "            pass\n",
    "\n",
    "    return np.exp(- logprob / N)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ekhHxEZOC0Ht"
   },
   "source": [
    "# 5) Implement Additive Laplace smoothing to give a non-zero probability to      missing prefix—token combinations when calculating perplexity. \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "bMPyoR4xALs2"
   },
   "outputs": [],
   "source": [
    "def perplexity_laplace(sentence,delta = 1):\n",
    "    sentence = tokenizer.tokenize(sentence.lower())\n",
    "    N = len(sentence)\n",
    "    logprob = 1\n",
    "    for ngram in ngrams(\n",
    "          sentence, \n",
    "          n= ngrams_degree,  \n",
    "          pad_right=True, pad_left=True, \n",
    "          left_pad_symbol=\"<s>\", right_pad_symbol=\"</s>\"):\n",
    "        prefix = ngram[:ngrams_degree-1]\n",
    "        token = ngram[ngrams_degree-1]\n",
    "        if prefix in list(counts.keys()):\n",
    "            total = sum( counts[prefix].values()  )\n",
    "            if token in counts[prefix].keys():\n",
    "                # normal calculation\n",
    "                logprob += np.log( (counts[prefix][token] + delta)/ (total + delta * N ) )\n",
    "            else:\n",
    "                logprob += np.log( ( delta)/ (total + delta * N ) )\n",
    "        else:\n",
    "            logprob += - np.log( N )\n",
    "  \n",
    "    return np.exp(-logprob / N)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "k008Cdzt0VH5"
   },
   "source": [
    "# 6) Calculate the perplexity of the language model on the test set composed of titles.\n",
    "\n",
    "\n",
    "\n",
    "Instead of using laplace smoothing to deal with the missing bigrams and tokens, we will simply skip missing elements to make the function faster.\n",
    "Implementing laplace smoothing requires several extra conditions that are taking too much time to run.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "S9DMcOnEIAcp"
   },
   "outputs": [],
   "source": [
    "def logproba_sentence(sentence, delta = 1):\n",
    "    sentence = tokenizer.tokenize(sentence.lower())\n",
    "    logprob = 0\n",
    "    for ngram in ngrams(\n",
    "        sentence, n= ngrams_degree,  \n",
    "        pad_right=True, pad_left=True, \n",
    "        left_pad_symbol=\"<s>\", right_pad_symbol=\"</s>\"):\n",
    "        prefix = ngram[:ngrams_degree-1]\n",
    "        token = ngram[ngrams_degree-1]\n",
    "        try:\n",
    "          logprob += np.log( freq[prefix][token] )\n",
    "        except:\n",
    "          pass\n",
    "\n",
    "    return logprob\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "xv0NB4Qz3T0j"
   },
   "source": [
    "We can now implement the perplexity for a whole set of sentences\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "import decimal\n",
    "\n",
    "# Precision to use\n",
    "decimal.getcontext().prec = 10\n",
    "decimal.getcontext().Emax = 10**10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "J2vZmwl5xajz"
   },
   "outputs": [],
   "source": [
    "def corpus_perplexity(corpus):\n",
    "  # start by calculating the total number of tokens in the corpus\n",
    "  all_sentences = ' '.join(corpus)\n",
    "\n",
    "  all_tokens =  tokenizer.tokenize(all_sentences.lower())\n",
    "  N = len(tokens)\n",
    "\n",
    "  logprob = 0\n",
    "  for sentence in tqdm(corpus):\n",
    "    logprob += logproba_sentence(sentence)\n",
    "  \n",
    "  \n",
    "  return np.exp(decimal.Decimal(- logprob / N))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████| 1000/1000 [00:00<00:00, 3294.50it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Decimal('8.585001117E+13786')"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpus = df_test.text.sample(1000, random_state = 8).values\n",
    "corpus_perplexity(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████| 83685/83685 [03:06<00:00, 449.08it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Decimal('1.400876133E+1156070')"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# and the perplexity of the whole test corpus\n",
    "corpus_perplexity(df_test.text.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "LPLMDL Task 2",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
